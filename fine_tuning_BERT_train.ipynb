{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "file_path = \"/Users/YannanGao/NLP_Project2/project-files/train.json\"\n",
    "\n",
    "\n",
    "file = open(file_path)\n",
    "content = json.load(file)\n",
    "print(len(content))\n",
    "# print(content[\"train-0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168\n",
      "1168\n",
      "why houston flooding isn‘t a sign of climate change\n",
      "Distinguished US climate scientist, Dr Roy Spencer writes: \"In the context of climate change, is what we are seeing in Houston a new level of disaster which is becoming more common? The flood disaster unfolding in Houston is certainly very unusual. But so are other natural weather disasters, which have always occurred and always will occur....Major floods are difficult to compare throughout history because of the ways we alter the landscape., For example, as cities like Houston expand over the years, soil is covered up by roads, parking lots and buidings, with water rapidly draining off rather than soaking into the soil. The population of Houston is now ten times what is was in the 1920s.  The Houston metroplex has expanded greatly and the water drainage is basically in the direction of downtown Houston.\"\n"
     ]
    }
   ],
   "source": [
    "## All the text information stored in context_list\n",
    "context_list = []\n",
    "label_list = []\n",
    "for i in range(len(content)):\n",
    "    title = \"train-{}\".format(i)\n",
    "    context_list.append(content[title][\"text\"])\n",
    "    label_list.append(content[title][\"label\"])\n",
    "print(len(context_list))\n",
    "print(len(label_list))\n",
    "print(context_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168\n",
      "why houston flooding isn‘t a sign of climate change. Distinguished US climate scientist, Dr Roy Spencer writes: \"In the context of climate change, is what we are seeing in Houston a new level of disaster which is becoming more common? The flood disaster unfolding in Houston is certainly very unusual. But so are other natural weather disasters, which have always occurred and always will occur....Major floods are difficult to compare throughout history because of the ways we alter the landscape., For example, as cities like Houston expand over the years, soil is covered up by roads, parking lots and buidings, with water rapidly draining off rather than soaking into the soil. The population of Houston is now ten times what is was in the 1920s.  The Houston metroplex has expanded greatly and the water drainage is basically in the direction of downtown Houston.\"\n"
     ]
    }
   ],
   "source": [
    "# Combine the title with the content\n",
    "import re\n",
    "\n",
    "def reformat_content(contents):\n",
    "    concat_contents = []\n",
    "    for content in contents:\n",
    "        concat_content = re.sub(\"\\n\", \". \", content)\n",
    "        concat_contents.append(concat_content)\n",
    "    return concat_contents\n",
    "\n",
    "re_contents = reformat_content(context_list)\n",
    "print(len(re_contents))\n",
    "print(re_contents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(407, 2)\n",
      "546\n"
     ]
    }
   ],
   "source": [
    "# Expand the training data set with external data\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "negative_data = pd.read_csv(\"text.csv\")\n",
    "print(negative_data.shape)\n",
    "\n",
    "negative_content = []\n",
    "for i in range(len(negative_data)):\n",
    "    text = negative_data.iloc[i, 1]\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    if len(tokens) >= 50:\n",
    "        negative_content.append(text)\n",
    "negative_content.extend(negative_content * 2)\n",
    "print(len(negative_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syria signs Paris Climate Agreement leaving the US as the only country in the world not in it. Syria has become a signatory of the Paris climate agreement, leaving the US as the only country in the world not to support the framework deal to combat greenhouse gas emissions. When President Donald Trump announced he intended to pull the US out of the agreement, it initially meant America would join Nicaragua and Syria on a small list of countries who were not part of the deal.\n"
     ]
    }
   ],
   "source": [
    "print(negative_content[370])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "546\n",
      "1168\n"
     ]
    }
   ],
   "source": [
    "def normalize(corpus):\n",
    "    normalized_corpus = []\n",
    "    punctuation = \"*\"\n",
    "    for text in corpus:\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"<br />\", r\" \", text)\n",
    "        text = re.sub(' +', ' ', text)\n",
    "        text = re.sub(r'(\\W)(?=\\1)', ' ', text)\n",
    "        text = re.sub(r\"[|\\t]\", ',', text) \n",
    "        text = re.sub(r'\\s\\s+', ' ', text).lstrip(' ')\n",
    "        text = re.sub(r'[{}]+'.format(punctuation),' ',text)\n",
    "        \n",
    "        normalized_corpus.append(text)\n",
    "    return normalized_corpus\n",
    "\n",
    "external_neg_norm_data = normalize(negative_content)\n",
    "print(len(external_neg_norm_data))\n",
    "re_contents_norm = normalize(re_contents)\n",
    "print(len(re_contents_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the heartland institute recently released a 52-page report by jay lehr, ph.d., the organization’s science director, and 18 distinguished climate and weather researchers, providing an extensive, detailed critique of the november 2017 “climate science special report” (cssr) produced by the u.s. global change research program (usgcrp) . the taskforce assembled by lehr shows cssr suffers from the same “shortcomings and biases apparent in the previous work produced by the usgcrp.” among the fallacies cssr perpetuates is the misperception “the science is settled” that human activities are driving climate change and climate change is necessarily catastrophic for humanity and necessitates a wholesale takeover by governments to reshape the economy sans the use of fossil fuels . among the “foundational problems” with cssr is the fact its authors hide the myriad scientific uncertainties about the causes and consequences of ongoing climate change. multiple lines of evidence show the climate is much less sensitive to additional inputs of greenhouse gases than cssr’s authors assume. lehr’s team demonstrates cssr’s report downplays our inadequate understanding of the role clouds, oceanic cycles, and solar cycles play in climate change. in addition, the climate projections used by cssr rely on unproven assumptions about various feedback mechanisms kicking in to spike temperatures, while ignoring the fact the claimed feedbacks have not been shown to be active in the real world but are only climate model assumptions . the climate models cssr uses have proven faulty time and again. the climate models fail to reflect actual temperatures. they have predicted much more warming than the earth has experienced, and they failed to reflect the recent 18-year pause in temperature increases. in addition, cssr can produce no solid data confirming climate model projections of declining ice cover in antarctica, nor for worsening hurricanes, droughts, and floods. in short, as the heartland institute’s taskforce reports shows, cssr places too much faith in computer models while neglecting to apply the scientific method . in addition to providing a detailed critique of the misstatements in the cssr, the heartland institute’s report discusses basic facts about the role of carbon dioxide in climate change which are completely ignored in cssr, and it shows the earth has been warmer in the past and coastlines are not facing runaway rising sea levels . one could find no better summary of the numerous flaws and outright falsehoods contained in the obama administration’s parting climate scare story, the cssr, than this report from the heartland institute. i recommend reading it in full . — h. sterling burnett. source: the heartland institute. climate change is not a catastrophe . countries falling short of paris climate commitments . noaa caught manipulating data again. climate change not a catastrophe. recent articles in scientific american and elsewhere acknowledge what many climate realists have long said: climate change, whether human-caused or not, does not mean the end of human civilization or life on earth. instead, human life is getting better. in one essay, journalist will boisvert argues the climate influence of human greenhouse gas emissions isn’t catastrophic, writing:. how bad will climate change be? not very. … while the climate upheaval will be large, the consequences for human well-being will be small. looked at in the broader context of economic development, climate change will barely slow our progress in the effort to raise living standards . among the evidence boisvert cites to prove his case is a lancet study released in 2016 which the mainstream media portrayed as showing climate change would cause food shortages leading to 529,000 deaths each year from malnutrition and related diseases. what the study really showed is,. … in 2050 the world will be better fed than ever before. … [f]ood will be more abundant than now thanks to advances in agricultural productivity that will dwarf the effects of climate change, . rais[ing] per-capita food availability to 3,107 kilocalories per day, … substantially higher than the benchmarked 2010 level of 2,817 kilocalories—and for a much larger global population. the poorest countries will benefit most, with food availability rising 14 percent in africa and southeast asia. [t]he study estimates that improved diets will save a net 1,348,000 lives per year in 2050 . in addition, boisvert notes environmentalists link syria’s civil war to the drought experienced in the region from 2006 through 2010, citing it as a harbinger of climate crises to come. yet israel suffered the same drought but overcame it, without a civil war, boisvert points out. technological breakthroughs in desalination allowed the country to reduce the energy needed to desalinate sea water by 50 percent, dramatically lowering the cost of doing so, with the result being “israel’s water situation u-turned from worsening scarcity to sufficiency.”. a separate article in scientific american discusses harvard scientist steven pinker’s efforts to get environmentalists to admit the tremendous benefits delivered by modern technologies using fossil fuels . “[i]ndustrialization has been good for humanity. it has fed billions, doubled lifespans, slashed extreme poverty, and, by replacing muscle with machinery, made it easier to end slavery, emancipate women, and educate children,” writes pinker. “it has allowed people to read at night, live where they want, stay warm in winter, see the world, and multiply human contact. any costs in pollution and habitat loss have to be weighed against these gifts. … cleaner is better, but not at the expense of everything else in life.”. citing yale university’s environmental performance index, pinker writes,. the wealthier the country, on average, the cleaner its environment: the nordic countries were cleanest; afghanistan, bangladesh, and several sub-saharan african countries, the most compromised. two of the deadliest forms of pollution—contaminated drinking water and indoor cooking smoke—are afflictions of poor countries. but as poor countries have gotten richer in recent decades, they are escaping these blights: the proportion of the world’s population that drinks tainted water has fallen by five-eighths, the proportion breathing cooking smoke by a third.”. climate realists have made similar arguments for decades. to quote the immortal john mcclane (die hard), “welcome to the party, pal!”. sources: scientific american; progress and peril; the breakthrough. barely two years after leaders from 195 countries announced a global agreement to reduce carbon dioxide emissions, those emissions are rising, not falling, as multiple countries are finding it difficult to make the cuts they promised in paris in 2015, the washington post reports . quoting robert jackson, a senior fellow at the stanford woods institute for the environment, the post notes governments failing to meet carbon dioxide emission reduction goals is not a new problem . “‘more than two decades ago, the world agreed to stabilize greenhouse gas concentrations in our air to prevent dangerous climate outcomes,’ said jackson, … referring to the 1992 framework convention on climate change that set international negotiations in motion. ‘to date, we have failed,’” the post reports . “‘tremendous gains in energy efficiency and renewable power aren’t yet reducing our global hunger for fossil fuels, especially oil and natural gas,’ [jackson] added,” according to the post . greenhouse gas emissions are rising in both developing and developed countries. turkey, indonesia, and developing countries throughout asia and africa are building and planning new coal-fired power plants to generate electricity necessary for economic growth. forests remove carbon dioxide from the atmosphere and count toward some countries’ emission reduction targets under the paris agreement, yet brazil has been unable to prevent deforestation, which has actually risen, with 2016’s total forest loss of almost 8,000 square kilometers close to double the level seen in 2012 . many developed countries, including some that pushed the hardest for the paris agreement, are doing no better in meeting their goals. germany, which set an aggressive near-term goal of cutting greenhouses gas emissions by 40 percent below 1990 levels by the year 2020, admitted in early 2017 it would be unable to hit its target. greater use of coal caused germany’s emissions to increase in 2015 and 2016. the european union as a whole is far below the pace necessary to meet its 2030 target of cutting carbon dioxide emissions 40 percent cut below 1990 levels by 2030. china also recently admitted its carbon dioxide emissions had begun to rise again, after remaining stable for two years . source: the washington post. citing an analysis by paul homewood, who runs the climate blog “not a lot of people know that,” journalist james delingpole writes, “the national oceanic and atmospheric administration (noaa) has yet again been caught exaggerating ‘global warming’ by fiddling with the raw temperature data.”. using datasets from weather stations in new york state, homewood discovered noaa was writing recent record-breaking cold weather in late december 2017 and early january 2018 out of existence. the recent extreme cold spell along the u.s. atlantic coast was noted for freezing sharks to death in the ocean and causing frozen iguanas to drop dead out of trees. record daily lows and record daily low high temperatures were set in numerous locations along the eastern seaboard. yet you wouldn’t know that from examining noaa’s “adjusted” temperature records . homewood discovered noaa adjusted temperatures by as much as 3.1 degrees fahrenheit, making past temperatures look colder than actually recorded and recent temperatures look warmer than they were . using new york datasets from the national weather service (nws), homewood exposed noaa’s shenanigans, first comparing the winters of 1943 and 2014 and then comparing 1943 to the winter beginning in late 2017 . nws determined,. the winter of 2013-14 finished as one of the coldest winters in recent memory for new york state. snowfall across western and north central new york was above normal for many areas, and in some locations well above normal . temperatures this winter finished below normal every month, and the january through march timeframe finished at least 4 degrees below normal for the two primary climate stations of western new york. … relentless cold continued through the month of january across the region . yet homewood found noaa listed the 2013 through 2014 winter as only the 30th coldest winter since 1895 on its new york state charts. comparing local nws records for january 1943 and january 2014—months which noaa listed as having similar average temperatures—homewood found noaa’s charts were grossly in error. the agency adjusted 2014 local temperatures significantly upward while adjusting the 1943 temperatures downward. according to homewood’s analysis, “on average the mean temperatures in jan 2014 were 2.7 [degrees fahrenheit] less than in 1943. yet, according to noaa, the difference was only 0.9 [degrees fahrenheit].”. homewood did a similar comparison for the december 2017 through january 2018 freeze and found noaa had fiddled with the data again . according to noaa, january 2018 in new york was warmer than january 1943, yet the raw data from local stations tells a different story. looking at temperatures from weather stations in the central lakes region of new york, for example, homewood found noaa reports the average temperature for central lakes was 20.8 degrees fahrenheit in january 2018, 2.1 degrees warmer than the 18.7 degrees fahrenheit it reports for january 1943.according to new york state’s official climate reports, however, the region’s temperature was 20.6 f (1.9 degrees warmer than noaa reports) in 1943, and the average temperature for january 2018 was 19.6 f (1.2 degrees colder than noaa reports), one degree colder than 1943. noaa adjusted the overall temperatures by more than 3.1 degrees . based on this and other gross discrepancies between recorded temperatures and noaa’s adjusted data, homewood concludes, “clearly noaa’s highly … adjusted version of the central lakes temperature record bears no resemblance at all the actual station data. and if this one division is so badly in error, what confidence can there be that the rest of the us is any better?”. good question . sources: breitbart; not a lot of people know that\n"
     ]
    }
   ],
   "source": [
    "print(re_contents_norm[123])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the main contributor to warming over the last 170 years is human influence on climate from increasing greenhouse gases in the atmosphere,” the met office said on wednesday. global temperatures in 2019 were on average 1.05c above pre-industrial levels and this year could be hotter, the british weather service said.\n"
     ]
    }
   ],
   "source": [
    "print(external_neg_norm_data[123])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "# Randomly select 1168 neg samples\n",
    "import random\n",
    "\n",
    "select_neg_data = random.sample(external_neg_norm_data, 500)\n",
    "select_pos_data = random.sample(re_contents_norm, 500)\n",
    "print(len(select_neg_data))\n",
    "print(len(select_pos_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "# Concat the positive data with the external negative data\n",
    "train_contents = select_pos_data + select_neg_data\n",
    "print(len(train_contents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "# Concat the negative labels into the label_list\n",
    "train_label_list = []\n",
    "train_label_list.extend([1] * len(select_pos_data))\n",
    "train_label_list.extend([0] * len(select_neg_data))\n",
    "print(len(train_label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "content_length = []\n",
    "for content in re_contents:\n",
    "    tokens = nltk.word_tokenize(content)\n",
    "    content_length.append(len(tokens))\n",
    "print(content_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize = (10, 10))\n",
    "plt.hist(content_length, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (2.8.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from transformers) (4.32.1)\n",
      "Requirement already satisfied: boto3 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from transformers) (1.9.152)\n",
      "Requirement already satisfied: tokenizers==0.5.2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: sentencepiece in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from transformers) (0.1.86)\n",
      "Requirement already satisfied: sacremoses in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from transformers) (0.0.41)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from transformers) (2019.4.14)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from transformers) (1.15.4)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from transformers) (2.21.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from boto3->transformers) (0.9.4)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from boto3->transformers) (0.2.0)\n",
      "Requirement already satisfied: botocore<1.13.0,>=1.12.152 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from boto3->transformers) (1.12.152)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from sacremoses->transformers) (7.0)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: six in /Users/troycao/Library/Python/3.7/lib/python/site-packages (from sacremoses->transformers) (1.12.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests->transformers) (1.24.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests->transformers) (2018.11.29)\n",
      "Requirement already satisfied: docutils>=0.10 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from botocore<1.13.0,>=1.12.152->boto3->transformers) (0.14)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /Users/troycao/Library/Python/3.7/lib/python/site-packages (from botocore<1.13.0,>=1.12.152->boto3->transformers) (2.7.5)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.7/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Now we gonna load the module\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/troycao/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "nltk.download('punkt')\n",
    "sent_segmenter = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "# Here we gonna get the ids of all the contents\n",
    "\n",
    "def convert_para_to_id(contents, para_length):\n",
    "    contents_ids_list = []\n",
    "    attentions_list = []\n",
    "    for content in contents:\n",
    "        word_count = 0\n",
    "        content_list = []\n",
    "        attention_list = []\n",
    "        sentences = sent_segmenter.tokenize(content)\n",
    "        for sentence in sentences:\n",
    "            while word_count < para_length:\n",
    "                sentence = re.sub(r\"[^a-zA-Z0-9]\", \"\", sentence)\n",
    "                encoded_con = tokenizer.encode(sentence, add_special_tokens=True)\n",
    "                content_list.extend(encoded_con)\n",
    "                word_count += len(encoded_con)\n",
    "        if len(content_list) > para_length:\n",
    "            content_list = content_list[:para_length]\n",
    "            attention_list.extend([1] * para_length)\n",
    "        else:\n",
    "            content_list.extend([0] * (para_length - len(content_list)))\n",
    "            attention_list.extend([1] * len(content_list))\n",
    "            attention_list.extend([0] * (para_length - len(content_list)))\n",
    "        contents_ids_list.append(content_list)\n",
    "        attentions_list.append(attention_list)\n",
    "    return contents_ids_list, attentions_list\n",
    "\n",
    "ids_list, attention_list = convert_para_to_id(train_contents, 50)\n",
    "print(len(ids_list))\n",
    "print(len(attention_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covert label list to Tensor\n",
    "import torch\n",
    "\n",
    "labels = torch.LongTensor(train_label_list)\n",
    "input_ids = torch.LongTensor(ids_list)\n",
    "attention_mask = torch.LongTensor(attention_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "# Combine the traiing inputs into a TensroDataset\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "train_data = TensorDataset(input_ids, attention_mask, labels)\n",
    "\n",
    "\n",
    "train_data_loader = DataLoader(train_data, sampler=RandomSampler(train_data), batch_size=10)\n",
    "print(len(train_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Model\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-uncased\",\n",
    "        num_labels = 2,\n",
    "        output_attentions = False,\n",
    "        output_hidden_states = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr = 2e-5, eps=1e-8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "total_steps = len(train_data_loader) * epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a helper function for calculating Accuracy\n",
    "import numpy as np\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "# get the dev data\n",
    "def read_dev_file(file_path):\n",
    "    file = open(file_path)\n",
    "    contents_list = []\n",
    "    labels_list = []\n",
    "    content = json.load(file)\n",
    "    for i in range(len(content)):\n",
    "        title = \"dev-{}\".format(i)\n",
    "        contents_list.append(content[title][\"text\"])\n",
    "        labels_list.append(content[title][\"label\"])\n",
    "    return contents_list, labels_list\n",
    "\n",
    "dev_file_path = \"/Users/troycao/Documents/GitHub/NLP_Project2/project-files/dev.json\"\n",
    "dev_contents_list, dev_labels_list = read_dev_file(dev_file_path)\n",
    "\n",
    "dev_re_contents = reformat_content(dev_contents_list)\n",
    "dev_re_contents_norm = normalize(dev_re_contents)\n",
    "print(len(dev_re_contents_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_ids_list, dev_attention_list = convert_para_to_id(dev_re_contents_norm, 50)\n",
    "\n",
    "dev_labels = torch.LongTensor(dev_labels_list)\n",
    "\n",
    "dev_input_ids = torch.LongTensor(dev_ids_list)\n",
    "dev_attention_mask = torch.LongTensor(dev_attention_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "dev_data = TensorDataset(dev_input_ids, dev_attention_mask, dev_labels)\n",
    "\n",
    "\n",
    "dev_data_loader = DataLoader(dev_data, sampler=RandomSampler(dev_data), batch_size=10)\n",
    "print(len(dev_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 2 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:40: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10 of 100\n",
      "Batch 20 of 100\n",
      "Batch 30 of 100\n",
      "Batch 40 of 100\n",
      "Batch 50 of 100\n",
      "Batch 60 of 100\n",
      "Batch 70 of 100\n",
      "Batch 80 of 100\n",
      "Batch 90 of 100\n",
      "\n",
      " Average Training Loss is 0.609810\n",
      "\n",
      "Running Validation...\n",
      "Validation loss :0.9584946751594543\n",
      "Accuracy is 0.48999999999999994\n",
      "\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "Batch 10 of 100\n",
      "Batch 20 of 100\n",
      "Batch 30 of 100\n",
      "Batch 40 of 100\n",
      "Batch 50 of 100\n",
      "Batch 60 of 100\n",
      "Batch 70 of 100\n",
      "Batch 80 of 100\n",
      "Batch 90 of 100\n",
      "\n",
      " Average Training Loss is 0.488814\n",
      "\n",
      "Running Validation...\n",
      "Validation loss :1.324871677160263\n",
      "Accuracy is 0.4800000000000001\n",
      "\n",
      "Training Complete!...\n"
     ]
    }
   ],
   "source": [
    "# Now training\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "\n",
    "training_stats = [] # used to store the training information\n",
    "\n",
    "\n",
    "for epoch_i in range(epochs):\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"======== Epoch {:} / {:} ========\".format(epoch_i + 1, epochs))\n",
    "    print(\"Training...\")\n",
    "    \n",
    "    total_train_loss = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for step, batch in enumerate(train_data_loader):\n",
    "        \n",
    "        if step % 10 == 0 and not step == 0:\n",
    "            print(\"Batch {} of {}\".format(step, len(train_data_loader)))\n",
    "        \n",
    "        batch_input_ids = torch.LongTensor(batch[0])\n",
    "        batch_input_mask = torch.LongTensor(batch[1])\n",
    "        batch_labels = batch[2]\n",
    "        \n",
    "        model.zero_grad()\n",
    "        loss, logits = model(batch_input_ids, token_type_ids=None,\n",
    "                            attention_mask=batch_input_mask,\n",
    "                            labels=batch_labels)\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(), 1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        scheduler.step()\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_data_loader)\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\" Average Training Loss is {:2f}\".format(avg_train_loss))\n",
    "    \n",
    "    # Now perform validation\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    \n",
    "    for batch in dev_data_loader:\n",
    "        dev_b_input_ids = batch[0]\n",
    "        dev_b_input_mask = batch[1]\n",
    "        dev_b_labels = batch[2]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            loss, logits = model(dev_b_input_ids, \n",
    "                                 token_type_ids=None, \n",
    "                                attention_mask=dev_b_input_mask,\n",
    "                                labels=dev_b_labels)\n",
    "        total_eval_loss += loss.item()\n",
    "        logits = logits.detach().numpy()\n",
    "        label_ids = dev_b_labels.numpy()\n",
    "        \n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "    avg_val_loss = total_eval_loss / len(dev_data_loader)\n",
    "    avg_val_accuracy = total_eval_accuracy / len(dev_data_loader)\n",
    "    print(\"Validation loss :{}\".format(avg_val_loss))\n",
    "    print(\"Accuracy is {}\".format(avg_val_accuracy))\n",
    "    \n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy\n",
    "        }\n",
    "    )\n",
    "    \n",
    "print(\"\")\n",
    "print(\"Training Complete!...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"BERT_v6.0.1pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('precision', 2)\n",
    "\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "\n",
    "# Use the 'epoch' as the row index.\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "\n",
    "df_stats.to_csv(\"Bert_version5.0.csv\", index=True, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
      "        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,\n",
      "        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1])\n",
      "The Precision is: 0.9400\n",
      "The Recall is: 0.5663\n",
      "The Macro_F1_Score is: 0.5623\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "bert_test_model = torch.load(\"./meta_data/BERT_v6.0.pkl\")\n",
    "\n",
    "bert_test_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_loss, test_logits = bert_test_model(dev_input_ids, token_type_ids=None,\n",
    "                            attention_mask=dev_attention_mask,\n",
    "                            labels=dev_labels)\n",
    "test_pred = np.argmax(test_logits, axis=1).flatten()\n",
    "actual_pred = dev_labels.flatten()\n",
    "print(test_pred)\n",
    "\n",
    "p = precision_score(test_pred, actual_pred)\n",
    "print(\"The Precision is: {:.4f}\".format(p))\n",
    "\n",
    "r = recall_score(test_pred, actual_pred)\n",
    "print(\"The Recall is: {:.4f}\".format(r))\n",
    "\n",
    "f1_score = f1_score(test_pred,actual_pred, average='macro')\n",
    "print(\"The Macro_F1_Score is: {:.4f}\".format(f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,\n",
      "        0, 1, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "print(actual_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load test data\n",
    "def read_test_file(file_path):\n",
    "    file = open(file_path)\n",
    "    contents_list = []\n",
    "    labels_list = []\n",
    "    content = json.load(file)\n",
    "    for i in range(len(content)):\n",
    "        title = \"test-{}\".format(i)\n",
    "        contents_list.append(content[title][\"text\"])\n",
    "    return contents_list\n",
    "\n",
    "test_content = read_test_file(\"/Users/troycao/Documents/GitHub/NLP_Project2/project-files/test-unlabelled.json\")\n",
    "test_re_contents = reformat_content(test_content)\n",
    "test_re_contents_norm = normalize(test_re_contents)\n",
    "\n",
    "print(len(test_re_contents_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids_list, test_attention_list = convert_para_to_id(test_re_contents_norm, 80)\n",
    "\n",
    "test_input_ids = torch.LongTensor(test_ids_list)\n",
    "test_attention_mask = torch.LongTensor(test_attention_list)\n",
    "temp_labels = []\n",
    "temp_labels.extend([1] * len(test_re_contents_norm))\n",
    "test_labels = torch.LongTensor(temp_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = torch.load(\"./meta_data/BERT_v6.0.1pkl\")\n",
    "test_model.eval()\n",
    "with torch.no_grad():\n",
    "    test_loss, test_logits = test_model(test_input_ids, token_type_ids=None,\n",
    "                            attention_mask=test_attention_mask,\n",
    "                            labels=test_labels)\n",
    "\n",
    "test_pred = np.argmax(test_logits, axis=1).flatten()\n",
    "test_result = {}\n",
    "\n",
    "for i in range(len(test_pred)):\n",
    "    title = \"test-{}\".format(i)\n",
    "    test_result[title] = {}\n",
    "    test_result[title][\"label\"] = test_pred[i]\n",
    "print(test_result[\"test-0\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_result)):\n",
    "    title = \"test-{}\".format(i)\n",
    "    item = test_result[title][\"label\"]\n",
    "    test_result[title][\"label\"] = int(item.numpy())\n",
    "print(test_result[\"test-0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test-output.json\", \"w\") as f:\n",
    "     json.dump(test_result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
